<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">


    <title>Mandolin</title>


    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Mandolin" />
    <meta name="keywords" content="machine learning, distributed" />


    <link href="../css/style.css" rel="stylesheet" type="text/css">
    <script src="../js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!-- 
    <link href="../css/bootstrap.css" rel="stylesheet">
    <link href="../css/docs.css" rel="stylesheet">
    -->

    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->


  </head>
  
  <div id="container">
  <div id="header"> 
    <h1 id="logo-text">Mandolin</h1>
  </div>
  <div id="site-content">
  <div id="col-left">    
  <p id="top">
     <p>GLP models are specified through a simple JSON-based syntax that provides
     an ordered list of objects describing each layer in a feed-forward artificial
     neural network. </p>
     
     <h2 id="syntax" class="section">Syntax</h2>
     <p>Each layer specification must a layer type denoted with an <code>ltype</code> attribute.
     Additional options for the layer for regularization are then provided as part of the layer
     object, all denoted with JSON syntax.</p>
     <p>Example:</p>
     <pre><code>mandolin.trainer.specification = 
  [{&quot;ltype&quot;:&quot;Input&quot;, &quot;dropout-ratio&quot;:0.2},
   {&quot;ltype&quot;:&quot;Relu&quot;,&quot;dim&quot;:50,&quot;l1-pen&quot;:0.00001},
   {&quot;ltype&quot;:&quot;SoftMax&quot;,&quot;l1-pen&quot;:0.001}]</code></pre>
     <p>The above example specifies three layers - i.e. an input layer, an output layer
     of type <em>SoftMax</em> and a single hidden layer using Rectified Linear activations. 
     Additional options for each layer are specified through attribute value pairs using
     JSON syntax. For example, the above specification indicates 20% dropout (i.e. masking
     noise) for the input layer, and different L1 regularization penalty terms for the
     single hidden layer and output layer.</p>
     
     <h2 id="input-layers" class="section">Input Layers</h2>
     <p>Input layer types include <strong>Input</strong> and <strong>InputSparse</strong>, the latter denoting that 
     the input is sparse and sparse vector and matrix representations should be used
     for the inputs and weights between the first and second layers. </p>
     
     <h3 id="options" class="section">Options</h3>
     <p>The only additional specification for an input layer is <em>dropout-ratio</em>. This adds
     masking noise to non-zero features according to the ratio/percentage specified.</p>
     
     <h2 id="hidden-layers" class="section">Hidden Layers</h2>
     <p>Layer types include <strong>Logistic</strong>, <strong>TanH</strong>, <strong>Relu</strong> (Rectified linear units) and <strong>Linear</strong>. 
     These functions correspond to those described in the literature. 
     In addition to the type, for each hidden layer the dimensionality (i.e. number of nodes/neurons)
     must be specified.  This is done using the &quot;dim&quot; attribute. Each hidden layer can optionally
     be regularized in a few different ways:</p>
     
     <h3 id="options-1" class="section">Options</h3>
     <p><strong>dropout-ratio</strong>: Just as with input layers, the activations from previous layers can
     be &quot;dropped out&quot; randomly.  See the paper on this: <em>Dropout: A Simple Way to Prevent 
     Neural Networks from Overfitting</em>, Hinton et al..</p>
     <p><strong>l1-pen</strong>: This option specifies an L1 regularizer on the weights for this layer. Larger
     penalty terms introduce stronger regularization</p>
     <p><strong>l2-pen</strong>: This option specifies an L2 regularizer on the weights.  Currently only either
     L1 or L2 regularization is supported.  If both penalty terms are present, L1 regularization
     takes precedence and the L2 penalty will be ignored.</p>
     <p><strong>max-norm</strong>: Max-norm enforces that the L2 norm of the weights is less than the specified value;
     thus smaller values here provide stronger regularization.  This type of regularization is 
     especially useful in conjunction with Relu layers as the weights can receive large values as 
     the gradients don&#39;t &quot;vanish&quot; as with sigmoid-type activations.</p>
     
     <h2 id="output-layers" class="section">Output Layers</h2>
     <p>Currently, Mandolin is focused on classification algorithms rather than 
     regression models. Output layers are thus geared towards multiclass classification.
     The primary (default) output layer is the <em>SoftMax</em> layer, however other
     output layer types that provide different types of cost/loss functions.</p>
     <p><strong>SoftMax</strong>: Softmax layer implements a cross entropy loss where the linear output activations
     are transformed with the softmax function.</p>
     <p><strong>Hinge</strong>: This is an &quot;L1&quot; multiclass hinge loss as proposed by Crammer and Singer. </p>
     <p><strong>ModHuber</strong>: This is a modified Huber loss geared for classification problems, c.f.
     <em>Solving large scale linear prediction problems using stochastic gradient descent algorithms. ICML.</em>.</p>
     <p><strong>Ramp</strong>: This is a nonconvex loss function. It has a hinge loss but flattens out so that examples 
     &quot;strongly&quot; misclassified by the model do not contribute to the loss.  This effectively <em>ignores</em>
     hard-to-classify examples and can be useful in situations where significant lable noise is present.
     See <em>Trading Convexity for Scalability</em>, Collobert, Sinz, Weston, Bottou. (ICML 2006).</p>
     <p><strong>Translog</strong>: This is a smooth version of the ramp loss, non-convex</p>
     <p><strong>T-Logistic</strong>: Another smooth non-convex loss suitable for problems with label noise, cf. 
     <em>t-Logistic Regression</em>, Ding and Vishwanathan. </p>
     
     <h3 id="options-2" class="section">Options</h3>
     <p>Each output layer can take l1, l2 or max-norm regularization options. </p>      
  </p>
  </div>

  <div id="col-right">
    <!-- <div style="padding: 30px 10px 10px;">  -->
      <ul>
       <div class="toc">
         <ul>
           <li>
             <p class="toc level1">Overview</p>
             <ul>
               <li><a class="toc level2" href="../overview/intro.html">Introduction</a></li>
             </ul>
           </li>
           <li>
             <p class="toc level1">Users Guide</p>
             <ul>
               <li><a class="toc level2" href="installation.html">Installation</a></li>
               <li><a class="toc level2" href="quick-start.html">Quickstart</a></li>
               <li><a class="toc level2" href="formats.html">Formats</a></li>
               <li><a class="toc level2" href="configuration.html">Configuration</a></li>
               <li><a class="toc level2" href="mandolin-glp.html">Generalized Layered Perceptron</a></li>
               <li><span class="toc level2 active">Model Specification</span></li>
             </ul>
           </li>
           <li>
             <p class="toc level1">Quick-start Without Spark</p>
             <ul>
               <li><a class="toc level2" href="../without-spark-quickstart/linear-classifier.html">Linear Classifier</a></li>
             </ul>
           </li>
           <li>
             <p class="toc level1">Developer Docs</p>
             <ul>
               <li><a class="toc level2" href="../api-docs/api.html">API Docs</a></li>
             </ul>
           </li>
           <li>
             <p class="toc level1">Runtime Decoder</p>
             <ul>
               <li><a class="toc level2" href="../runtime-decoder/runtime-decoder.html">Runtime Decoder</a></li>
             </ul>
           </li>
         </ul>
       </div>
      </ul>
    </div>
  </div>



  </div>


</body></html>
